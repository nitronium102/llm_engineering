{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e61417",
   "metadata": {},
   "source": [
    "# Day 4\n",
    "\n",
    "## Tokenizing with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc1c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: tiktoken is OpenAI's tokenizer. For Ollama models (like llama3.2), \n",
    "# we can still use tiktoken with compatible encodings, or use the model's native tokenizer.\n",
    "# Here we'll use tiktoken with a compatible encoding for demonstration.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# For llama models, we can use cl100k_base encoding (same as GPT-3.5/GPT-4)\n",
    "# or o200k_base for newer models. Let's use cl100k_base which works well.\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokens = encoding.encode(\"Hi my name is Ed and I like banoffee pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7632966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13347, 856, 836, 374, 3279, 323, 358, 1093, 9120, 21869, 4447]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce0c188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13347 = Hi\n",
      "856 =  my\n",
      "836 =  name\n",
      "374 =  is\n",
      "3279 =  Ed\n",
      "323 =  and\n",
      "358 =  I\n",
      "1093 =  like\n",
      "9120 =  ban\n",
      "21869 = offee\n",
      "4447 =  pie\n"
     ]
    }
   ],
   "source": [
    "for token_id in tokens:\n",
    "    token_text = encoding.decode([token_id])\n",
    "    print(f\"{token_id} = {token_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e3bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([13347])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538efe61",
   "metadata": {},
   "source": [
    "# And another topic!\n",
    "\n",
    "### The Illusion of \"memory\"\n",
    "\n",
    "Many of you will know this already. But for those that don't -- this might be an \"AHA\" moment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a4b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Check if Ollama is running\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Ollama is running!\")\n",
    "    else:\n",
    "        print(\"Ollama server responded but with unexpected status. Please check if Ollama is running properly.\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"Ollama is not running. Please open a terminal and run 'ollama serve'\")\n",
    "    print(\"Then in another terminal, run 'ollama pull llama3.2' to download the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618859b",
   "metadata": {},
   "source": [
    "### You should be very comfortable with what the next cell is doing!\n",
    "\n",
    "_I'm creating a new instance of the OpenAI Python Client library, a lightweight wrapper around making HTTP calls to an endpoint. We're using it to call Ollama (which provides an OpenAI-compatible endpoint) running locally on your machine!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b959be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set up Ollama client using OpenAI-compatible interface\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa889e80",
   "metadata": {},
   "source": [
    "### A message to an LLM (via OpenAI-compatible API) is a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97298fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Ed!\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3475a36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ed! It's nice to meet you. Is there anything I can help you with today? Do you need assistance with a particular question or topic, or do you just want to chat? I'm all ears!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f45ed8",
   "metadata": {},
   "source": [
    "### OK let's now ask a follow-up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bce2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404462f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your name. We just started our conversation, and I don't retain any personal data. Would you like to share your name with me? I can help with anything related to it!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098237ef",
   "metadata": {},
   "source": [
    "### Wait, wha??\n",
    "\n",
    "We just told you!\n",
    "\n",
    "What's going on??\n",
    "\n",
    "Here's the thing: every call to an LLM is completely STATELESS. It's a totally new call, every single time. As AI engineers, it's OUR JOB to devise techniques to give the impression that the LLM has a \"memory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d43f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Ed!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Ed! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ac742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Ed. Or, at least, that's what we established earlier!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c49557",
   "metadata": {},
   "source": [
    "## To recap\n",
    "\n",
    "With apologies if this is obvious to you - but it's still good to reinforce:\n",
    "\n",
    "1. Every call to an LLM is stateless\n",
    "2. We pass in the entire conversation so far in the input prompt, every time\n",
    "3. This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a trick; it's a by-product of providing the entire conversation, every time\n",
    "5. An LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Ed\" and later \"What's my name?\" then it will predict.. Ed!\n",
    "\n",
    "The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in.\n",
    "\n",
    "\"Does that mean we have to pay extra each time for all the conversation so far\"\n",
    "\n",
    "With cloud APIs like OpenAI, yes - you pay for all the tokens in the conversation each time. With Ollama running locally, you don't pay API fees, but you still use compute resources (CPU/GPU) to process the entire conversation each time. The principle is the same: we want the LLM to predict the next tokens in the sequence, looking back on the entire conversation. We want that compute to happen!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b7c83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
